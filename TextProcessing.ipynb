{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15574e5b-8ba6-4833-83e8-92270df141cd",
   "metadata": {},
   "source": [
    "# 1 Text Cleaning/Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2ac79-0efd-4e07-a76b-30135428ad15",
   "metadata": {},
   "source": [
    "Before any text from open-ended questions can be used for matching, it needs to be cleaned and formatted.\n",
    "\n",
    "To start, we will use an example hobby/interests response for question #27 in the survey as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64068928-4d93-429b-864c-304711e28f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q27: Please let us know your favorite hobbies, and what you do on your days off for enjoyment.\n",
    "str1 = \"I enjoy spending a lot of my time cooking, baking, and hanging out with friends and family. I also like to hike when the weather is nice out. I attend tennis and dance classes every tuesday and thursday.\"\n",
    "\n",
    "#str1 = \"For privacy, I want a quiet room with a locked door on it. I would also feel safer if the house had security cameras setup and a keypad on the front door.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f848c7-403e-4045-b9a7-3c6188aeaac3",
   "metadata": {},
   "source": [
    "The goal is a list or set with the following words or a variation of the following words:\n",
    "cook, bake, hang, friend, family, hike, weather, tennis, dance, class\n",
    "<br>This list has a length of 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86830df2-911c-48d2-8224-8a10cd123c29",
   "metadata": {},
   "source": [
    "## 1.1 Preparing the text\n",
    "In the first stage, we need to do some basic text cleaning by changing all characters to lowercase and removing punctuation.\n",
    "#### 1.11 First, we can change the text to lowercase using the default python method .lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2067bfd-5e7c-4ba3-a691-25bcab3e5a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i enjoy spending a lot of my time cooking, baking, and hanging out with friends and family. i also like to hike when the weather is nice out. i attend tennis and dance classes every tuesday and thursday.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c12ab-3efc-4ebf-a5d6-56b716f7e1f7",
   "metadata": {},
   "source": [
    "#### 1.12 To remove unncessary punctuation, we need the assistance of the regular expression library, 're' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bad89c0-cef8-44c7-9959-860a9614e0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I enjoy spending a lot of my time cooking baking and hanging out with friends and family I also like to hike when the weather is nice out I attend tennis and dance classes every tuesday and thursday'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# first parameter for re.sub is what it is being replaced with the second parameter. third parameter is the input string.\n",
    "# \\w finds words, \\s finds white spaces, and ^ is used to select the opposite of words and white spaces which is all punctuation\n",
    "re.sub('[^\\w\\s]', '', str1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5008428-70f1-4f4b-af6c-9d7c080964e8",
   "metadata": {},
   "source": [
    "#### 1.13 Combining the two into a method. We do not have to import the 're' library again since it is already imported in 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4da37d-6bc9-4d0c-850a-e6354931fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = re.sub('[^\\w\\s]', '', clean_text)\n",
    "    \n",
    "    # this line replaces new line characters with whitespace if the user enters a new line when inputting their text\n",
    "    clean_text = re.sub('\\n', ' ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7a7cf-1050-485b-8d22-7ddd4f60ceb3",
   "metadata": {},
   "source": [
    "#### <br> 1.14 Testing our new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4938d8-a1e0-4971-bbdf-541b2de4d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i enjoy spending a lot of my time cooking baking and hanging out with friends and family i also like to hike when the weather is nice out i attend tennis and dance classes every tuesday and thursday\n"
     ]
    }
   ],
   "source": [
    "clean_text = text_cleaner(str1)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d3289-bd1e-4c3b-83af-2547e87c9a2e",
   "metadata": {},
   "source": [
    "#### 1.15 Changing the string to a list where each item in the list is a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2620343-5b9f-45bd-95cd-f4b96955e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'enjoy', 'spending', 'a', 'lot', 'of', 'my', 'time', 'cooking', 'baking', 'and', 'hanging', 'out', 'with', 'friends', 'and', 'family', 'i', 'also', 'like', 'to', 'hike', 'when', 'the', 'weather', 'is', 'nice', 'out', 'i', 'attend', 'tennis', 'and', 'dance', 'classes', 'every', 'tuesday', 'and', 'thursday']\n"
     ]
    }
   ],
   "source": [
    "clean_text_list = clean_text.split()\n",
    "print(clean_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89889c38-c073-4974-a682-14a5b3669590",
   "metadata": {},
   "source": [
    "## 1.2 Removing 'stop words'. \n",
    "These are words such as, 'i', 'the', 'a', etc. that do not have any meaning to us and are very common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01b3ff-2136-47b4-a0e6-1dacebd5bc0f",
   "metadata": {},
   "source": [
    "#### 1.21 Importing the NLTK (Natural Language Toolkit) library\n",
    "We will use this library to help us remove 'stop words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a9d44-a77b-4b7e-874b-8afad3f25a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use this to demonstrate some optimization\n",
    "import time\n",
    "\n",
    "# natural language toolkit library\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae628118-805a-4b6a-9a4e-2ca16078bfc2",
   "metadata": {},
   "source": [
    "#### 1.22 Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128952a2-4949-4b20-9060-60afad3770f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'like', 'also']\n"
     ]
    }
   ],
   "source": [
    "# list of english stopwords in lowercase\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "# words to add to the stopword list\n",
    "add_to_list = ['like', 'also']\n",
    "\n",
    "for word in add_to_list:\n",
    "    stopword_list.append(word)\n",
    "\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c141fc-c14a-49e3-8f2b-df93ce04bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'spending', 'lot', 'time', 'cooking', 'baking', 'hanging', 'friends', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'classes', 'every', 'tuesday', 'thursday']\n",
      "runtime: 4.4160120487213135 seconds\n"
     ]
    }
   ],
   "source": [
    "# removing the stopwords and timing it\n",
    "start = time.time()\n",
    "standard_list = []\n",
    "# for loop is only here to demonstrate time difference between set and list\n",
    "for x in range(1, 100000):\n",
    "    standard_list = [word for word in clean_text_list if word not in stopword_list]\n",
    "end = time.time()\n",
    "print(standard_list)\n",
    "print(\"runtime: \" + str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98296fec-da54-48a1-a46d-0985afc628ca",
   "metadata": {},
   "source": [
    "#### 1.23 Removing stopwords efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79438be-7152-4ab9-aa08-bddd1d9c9489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "For each word in our sentence list, we will need to check if it is in the stopword_list and remove it if it is. This can be a little time consuming if the text input from the user is large, but Python has a good solution for this, sets. Sets are similar to lists but are implemented using hash tables. This means it will use more memory but it will be way faster to search through since it has a searching time complexity of O(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b3f6284-a590-4f4a-8f81-4e68b8f77d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'until', \"wouldn't\", 'other', 'were', 'them', 'so', 'there', 'shouldn', 'myself', 'has', 'few', 're', 'herself', 'up', \"didn't\", \"isn't\", 'theirs', 'against', 'also', 'have', 'do', 'below', 'did', 'it', 'itself', 'isn', \"mightn't\", \"you'd\", \"don't\", 'o', \"aren't\", 'couldn', \"won't\", 'off', 've', \"shouldn't\", 'like', 'whom', 'yourselves', 'this', 'wouldn', 'some', 'before', 'on', 'hasn', 'himself', 'but', \"you're\", \"doesn't\", 'why', 'him', \"shan't\", 'am', 'd', 'not', 'her', 'what', 'which', 'its', 'most', 'over', 'through', 'such', \"it's\", 'again', 'your', 'with', 'to', 'ain', \"haven't\", \"should've\", 'during', 'their', 'about', 'hers', 'can', 'if', 'between', \"couldn't\", \"that'll\", 'those', 'will', 'the', 'and', 'too', \"hasn't\", 'after', 'out', 'at', 'll', 'wasn', 'nor', 'for', 's', 'his', 'our', 'while', 'further', 'under', 't', 'very', 'no', 'an', 'both', 'mustn', 'should', 'ma', \"wasn't\", 'down', 'she', \"she's\", 'above', 'these', 'of', 'how', 'shan', 'you', 'when', 'they', 'by', 'being', \"hadn't\", 'haven', 'm', 'we', 'here', \"weren't\", 'is', 'mightn', 'than', 'weren', \"you've\", 'once', 'yours', \"you'll\", 'any', 'won', 'all', 'don', 'each', 'y', 'where', 'or', 'having', 'does', 'own', 'yourself', \"mustn't\", 'needn', 'aren', 'a', 'that', 'just', 'had', 'same', 'me', 'because', 'who', 'doesn', 'my', 'into', 'only', 'now', 'was', 'be', 'i', 'in', 'are', 'then', 'more', 'themselves', 'hadn', 'ourselves', 'as', 'ours', 'he', 'didn', 'been', 'doing', \"needn't\", 'from'}\n"
     ]
    }
   ],
   "source": [
    "# converting the list to a set\n",
    "stopword_set = set(stopword_list)\n",
    "print(stopword_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d008cd-c57b-42d0-b9ab-b5387cb563ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'spending', 'lot', 'time', 'cooking', 'baking', 'hanging', 'friends', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'classes', 'every', 'tuesday', 'thursday']\n",
      "runtime: 0.1063838005065918 seconds\n"
     ]
    }
   ],
   "source": [
    "# removing the stopwords and timing it\n",
    "start = time.time()\n",
    "standard_list = []\n",
    "# for loop is only here to demonstrate time difference between set and list\n",
    "for x in range(1, 100000):\n",
    "    standard_list = [word for word in clean_text_list if word not in stopword_set]\n",
    "end = time.time()\n",
    "print(standard_list)\n",
    "print(\"runtime: \" + str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4f318-4e2e-45de-b674-c1aba4032d92",
   "metadata": {},
   "source": [
    "## 1.3 Removing Even More Words and Lemmatization\n",
    "Lemmatization is the process of reducing a word into its most basic form considering a language's full vocabulary. This is needed for matching because if we match without stemming, someone who writes \"I like baking.\" and someone who writes \"I like to bake.\" will end up not matching since the computer does not see the word \"baking\" the same as \"bake\". Thankfully, lemmatization will help us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58226b-65ec-4217-ac68-4a4a87e9129e",
   "metadata": {},
   "source": [
    "#### 1.31 More imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13814c7a-9473-4608-a0f3-ec879d443b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another great NLP library\n",
    "import spacy\n",
    "\n",
    "# and another NLP library\n",
    "from textblob import TextBlob\n",
    "\n",
    "# used by textblob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# load english processing tools \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0f030-f149-4ade-add1-d44a329dc356",
   "metadata": {},
   "source": [
    "#### 1.32 Lemmatize the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207aab89-9873-46d0-9479-7d7f35bf4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_form = ' '.join(standard_list)\n",
    "\n",
    "start = time.time()\n",
    "# create a Doc object\n",
    "doc = nlp((' '.join(standard_list)))\n",
    "\n",
    "\n",
    "# create tokens from the doc object, each representing a word\n",
    "token_list = []\n",
    "for token in doc:\n",
    "    token_list.append(token)\n",
    "    \n",
    "# lemmatize the list\n",
    "lemmatized_list = [token.lemma_ for token in token_list]\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f61406-7fce-40a3-97b5-a12f0b03f049",
   "metadata": {},
   "source": [
    "#### 1.33 Comparing before lemmatization and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc31187-5b24-42ab-b2af-5ee3a7346664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['enjoy', 'spending', 'lot', 'time', 'cooking', 'baking', 'hanging', 'friends', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'classes', 'every', 'tuesday', 'thursday']\n",
      "After: ['enjoy', 'spend', 'lot', 'time', 'cook', 'bake', 'hang', 'friend', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'class', 'every', 'tuesday', 'thursday']\n",
      "runtime: 0.00875234603881836 seconds to lemmatize\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \" + str(standard_list))\n",
    "print(\"After: \" + str(lemmatized_list))\n",
    "print(\"runtime: \" + str(end-start) + \" seconds to lemmatize\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b24db5-64c5-4a01-a8ed-bc727562eedf",
   "metadata": {},
   "source": [
    "#### 1.34 Extracting Notable Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2cb004-f8a5-45f6-a20c-95e0ed81094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['enjoy', 'spend', 'lot', 'time', 'cook', 'bake', 'hang', 'friend', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'class', 'every', 'tuesday', 'thursday']\n",
      "After: ['enjoy', 'spend', 'lot', 'time', 'cook', 'bake', 'hang', 'friend', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'class', 'tuesday', 'thursday']\n"
     ]
    }
   ],
   "source": [
    "blob_object = TextBlob(' '.join(lemmatized_list))\n",
    "\n",
    "#print(blob_object.tags)\n",
    "\n",
    "notable_words = [word for (word, pos) in TextBlob(' '.join(lemmatized_list)).pos_tags if pos[0] == 'N' or pos[0] == 'V' or pos[0] == 'J']\n",
    "\n",
    "print(\"Before: \" + str(lemmatized_list))\n",
    "print(\"After: \" + str(notable_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d955d-1cd6-4bd0-95ce-f70b01ccd5aa",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3e963-2999-4513-8d15-9adc543e6269",
   "metadata": {},
   "source": [
    "Started with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ac92e3-c164-47e6-851e-375162b9cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'enjoy', 'spending', 'a', 'lot', 'of', 'my', 'time', 'cooking,', 'baking,', 'and', 'hanging', 'out', 'with', 'friends', 'and', 'family.', 'I', 'also', 'like', 'to', 'hike', 'when', 'the', 'weather', 'is', 'nice', 'out.', 'I', 'attend', 'tennis', 'and', 'dance', 'classes', 'every', 'tuesday', 'and', 'thursday.']\n",
      "Length: 38\n"
     ]
    }
   ],
   "source": [
    "print(str(str1.split(' ')) + \"\\nLength: \" + str(len(str1.split(' ')))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c72d47-0ebd-474e-b5d4-a02edb8dcebf",
   "metadata": {},
   "source": [
    "Ended with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2656033c-f77a-4d26-8c3b-0581f321fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'spend', 'lot', 'time', 'cook', 'bake', 'hang', 'friend', 'family', 'hike', 'weather', 'nice', 'attend', 'tennis', 'dance', 'class', 'tuesday', 'thursday']\n",
      "Length: 18\n"
     ]
    }
   ],
   "source": [
    "print(str(notable_words) + \"\\nLength: \" + str(len(notable_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a4ba3-39ec-4477-b64a-3aa9148b9784",
   "metadata": {},
   "source": [
    "As you can see we are not down to the list of around 10 that is our goal, but it is still good progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300fa19-642a-45a3-a26d-1981a8b909d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
